<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Dynamic Deblurring Neural Radiance Fields for Blurry Monocular Video.">
  <meta name="keywords" content="DyBluRF, D-NeRF, NeRF, Deblur">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>From-Ground-To-Objects: Coarse-to-Fine Self-supervised Monocular Depth Estimation of Dynamic Objects with Ground Contact Prior</title>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-HR3PWVQLH2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-HR3PWVQLH2');
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" sizes="192x192" href="./static/images/icon.png" type="image/png"/>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/video_comparison.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero ">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="title is-1 publication-title"><h1 id="blur">From-Ground-To-Objects: </h1> <h1 style="display: inline; clear:none;">Coarse-to-Fine Self-supervised Monocular Depth Estimation of Dynamic Objects with Ground Contact Prior</h1></div>          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sites.google.com/view/jaehomoon">Jaeho Moon</a>&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/juan-luis-gb/home">Juan Luis Gonzalez Bello</a>&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://www.viclab.kaist.ac.kr/">Byeongjun Kwon</a>&nbsp;&nbsp;&nbsp;&nbsp;
            </span>
            <span class="author-block">
              <a href="https://www.viclab.kaist.ac.kr/">Munchurl Kim</a><sup>†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"> <sup>†</sup>Corresponding author</span>
            <!-- <span class="author-block"><sup>†</sup>Corresponding author</span> -->
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">KAIST</span>
            <!-- <span class="author-block"><sup>†</sup>Corresponding author</span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Moon_From-Ground-To-Objects_Coarse-to-Fine_Self-supervised_Monocular_Depth_Estimation_of_Dynamic_Objects_with_CVPR_2024_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.10118"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/KAIST-VICLab/From_Ground_To_Objects"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--<section class="hero is-light is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->
<!--        <div class="item item-paper-windmill">-->
<!--          <video poster="" id="paper-windmill" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/DyBluRF_1_paper-windmill.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-block">-->
<!--          <video poster="" id="block" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/DyBluRF_2_block.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-wheel">-->
<!--          <video poster="" id="wheel" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/DyBluRF_3_wheel.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<script>
bulmaCarousel.attach('#results-carousel', {
  slidesToShow: 2,
  loop: true,
  pagination: false,
});
</script>

<section class="section hero is-light">
  <div class="container is-widescreen">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
<!--        <h2 class="title is-3">Qualitative Results</h2>-->
        <!-- Deblurring. -->
        <h3 class="title is-4">

        </h3>
<!--        <div class="content has-text-justified">-->
<!--          <p>-->
<!--            Here, we compare the reconstructed depth maps of Monodepth2 and Ours-Monodepth2.-->
<!--          </p>-->
<!--        </div>-->
        <div class="row" style="display: flex; margin-bottom: 2%;">
          <div class="col-md-4 has-text-centered" style="width: 33%;">
              <video class="video" id="viz_input_image_1" loop playsinline autoplay muted controls src="./static/videos/demo_video_color.mp4" ></video>
            <span style="font-weight:bold;">Input Image</span>
          </div>
          <div class="col-md-4 has-text-centered" style="width: 33%;">
              <video class="video" id="viz_monodepth2" loop playsinline autoplay muted controls src="./static/videos/demo_video_monodepth2.mp4"></video>
            <span style="font-weight:bold;">Monodepth2</span>
          </div>
          <div class="col-md-4 has-text-centered" style="width: 33%;">
              <video class="video" id="viz_ours_monodepth2" loop playsinline autoplay muted controls src="./static/videos/demo_video_ours_monodepth2.mp4"></video>
            <span style="font-weight:bold;">Ours-Monodepth2</span>
          </div>
        </div>
        <div class="row" style="display: flex; margin-bottom: 2%;">
          <div class="col-md-4 has-text-centered" style="width: 33%;">
              <video class="video" id="viz_input_image_2" loop playsinline autoplay muted controls src="./static/videos/demo_video_hr_depth_color.mp4" ></video>
            <span style="font-weight:bold;">Input Image</span>
          </div>
          <div class="col-md-4 has-text-centered" style="width: 33%;">
              <video class="video" id="viz_hrdepth" loop playsinline autoplay muted controls src="./static/videos/demo_video_hr_depth.mp4"></video>
            <span style="font-weight:bold;">HR-Depth</span>
          </div>
          <div class="col-md-4 has-text-centered" style="width: 33%;">
              <video class="video" id="viz_ours_hrdepth" loop playsinline autoplay muted controls src="./static/videos/demo_video_ours_hr_depth.mp4"></video>
            <span style="font-weight:bold;">Ours-HR-Depth</span>
          </div>
        </div>
        <div class="row" style="display: flex; margin-bottom: 2%;">
          <div class="col-md-4 has-text-centered" style="width: 33%;">
              <video class="video" id="viz_input_image_3" loop playsinline autoplay muted controls src="./static/videos/demo_video_cadepth_color.mp4" ></video>
            <span style="font-weight:bold;">Input Image</span>
          </div>
          <div class="col-md-4 has-text-centered" style="width: 33%;">
              <video class="video" id="viz_cadepth" loop playsinline autoplay muted controls src="./static/videos/demo_video_cadepth.mp4"></video>
            <span style="font-weight:bold;">CADepth</span>
          </div>
          <div class="col-md-4 has-text-centered" style="width: 33%;">
              <video class="video" id="viz_ours_cadepth" loop playsinline autoplay muted controls src="./static/videos/demo_video_ours_cadepth.mp4"></video>
            <span style="font-weight:bold;">Ours-CADepth</span>
          </div>
        </div>
        <div class="row" style="display: flex; margin-bottom: 2%;">
          <div class="col-md-4 has-text-centered" style="width: 33%;">
              <video class="video" id="viz_input_image_4" loop playsinline autoplay muted controls src="./static/videos/demo_video_monovit_color.mp4" ></video>
            <span style="font-weight:bold;">Input Image</span>
          </div>
          <div class="col-md-4 has-text-centered" style="width: 33%;">
              <video class="video" id="viz_monovit" loop playsinline autoplay muted controls src="./static/videos/demo_video_monovit.mp4"></video>
            <span style="font-weight:bold;">MonoViT</span>
          </div>
          <div class="col-md-4 has-text-centered" style="width: 33%;">
              <video class="video" id="viz_ours_monovit" loop playsinline autoplay muted controls src="./static/videos/demo_video_ours_monovit.mp4"></video>
            <span style="font-weight:bold;">Ours-MonoViT</span>
          </div>
        </div>

      </div>
    </div>
    <!--/ Animation. -->


  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          Self-supervised monocular depth estimation (DE) is an approach to learning depth without costly depth ground truths.
          However, it often struggles with moving objects that violate the static scene assumption during training.
          To address this issue, we introduce a coarse-to-fine training strategy leveraging the ground contacting prior based on the observation that most moving objects in outdoor scenes contact the ground.
          In the coarse training stage, we exclude the objects in dynamic classes from the reprojection loss calculation to avoid inaccurate depth learning.
          To provide precise supervision on the depth of the objects, we present a novel Ground-contacting-prior Disparity Smoothness Loss (GDS-Loss) that encourages a DE network to align the depth of the objects with their ground-contacting points.
          Subsequently, in the fine training stage, we refine the DE network to learn the detailed depth of the objects from the reprojection loss, while ensuring accurate DE on the moving object regions by employing our regularization loss with a cost-volume-based weighting factor.
          Our overall coarse-to-fine training strategy can easily be integrated with existing DE methods without any modifications, significantly enhancing DE performance on challenging Cityscapes and KITTI datasets, especially in the moving object regions.

        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    </br>
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/-pOJ1g01G6o?si=YEZAclZwGrIWRObp"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
</section>

<section class="section">
  <div class="container is-widescreen">
    <h2 class="title is-3">Method Overview</h2>

    <div class="content has-text-justified">
      <p>
        We propose a novel coarse-to-fine training strategy to effectively handle moving object problems in self-supervised learning of monocular depth estimation.
        </br>
        In the coarse training stage, we utilize the ground contacting prior as a self-supervision for depth estimation on dynamic objects, presenting Ground-contacting-prior Disparity Smoonthess Loss (GDS-Loss).
        The ground contacting prior is based on an observation that most objects classified as dynamic in outdoor scenes, such as cars, bicycles, or pedestrians, invariably tend to make contact with the ground, thereby sharing similar depth at their ground contact points.
        </br>
        In the fine training stage, we further refine the depth estimation network to capture detailed depth of dynamic objects.
        We introduce a regularization loss with a cost-volume-based weighting factor to avoid inaccurate learning from the reproejction loss on the moving object regions.
      </p>
    </div>
    <div class="columns is-centered">
      <div class="column">
        <div class="columns is-centered">
          <div class="column content is-four-fifths">
            <img src="./static/images/main_architecture.png" alt="Graph for quality evaluation">
          </div>
        </div>
      </div>
    </div>
<!--    <div class="content has-text-justified">-->
<!--      <p>-->
<!--        We utilize the co-visibility masked image metrics, including mPSNR, mSSIM, and mLPIPS, -->
<!--        following the approach introduced by <a href="https://hangg7.com/dycheck/">Dycheck</a>. These metrics mask out the regions of the test video frames which are not observed by the training camera. -->
<!--        We further utilize <a href="https://arxiv.org/abs/1811.09393">tOF</a> to measure the temporal consistency of reconstructed video frames.-->
<!--      </p>-->
<!--    </div>-->
  </div>
</section>

<section class="section">
  <div class="container is-widescreen">
    <h2 class="title is-3">SOTA Comparison</h2>

    <div class="content has-text-justified">
      <p>
        Our coarse-to-fine training strategy significantly improves the depth estimation performance of existing methods including Monodepth2 [ICCV 2019], HR-Depth [AAAI 2021], CADepth [3DV 2021] and MonoViT [3DV 2022].
        Each model trained with our training strategy is denoted as Ours-Monodepth2, Ours-HR-Depth, Ours-CADepth, Ours-MonoViT.
        Our training strategy is easily integrated into those methods and enhances depth estimation performance on both Cityscapes and KITTI datasets.
        Moreover, Ours-MonoViT achieves state-of-the-art depth estimation performance on both datasets.
      </p>
    </div>
    <div class="columns is-centered">
      <div class="column">
        <div class="columns is-centered">
          <div class="column content is-two-thirds">
            <img src="./static/images/main_result.png" alt="Graph for quality evaluation">
          </div>
        </div>
      </div>
    </div>
<!--    <div class="content has-text-justified">-->
<!--      <p>-->
<!--        We utilize the co-visibility masked image metrics, including mPSNR, mSSIM, and mLPIPS, -->
<!--        following the approach introduced by <a href="https://hangg7.com/dycheck/">Dycheck</a>. These metrics mask out the regions of the test video frames which are not observed by the training camera. -->
<!--        We further utilize <a href="https://arxiv.org/abs/1811.09393">tOF</a> to measure the temporal consistency of reconstructed video frames.-->
<!--      </p>-->
<!--    </div>-->
  </div>
</section>


<section class="section">
  <div class="container is-widescreen">
    <h2 class="title is-3">3D Point Cloud Reconstructions</h2>

    <div class="content has-text-justified">

    </div>
    <div class="columns is-centered">
      <div class="column">
        <div class="columns is-centered">
          <div class="column content is-four-fifths">
            <img src="./static/images/point_cloud.png" alt="Graph for quality evaluation">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{moon2024ground,
      title={From-Ground-To-Objects: Coarse-to-Fine Self-supervised Monocular Depth Estimation of Dynamic Objects with Ground Contact Prior},
      author={Moon, Jaeho and Bello, Juan Luis Gonzalez and Kwon, Byeongjun and Kim, Munchurl},
      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
      pages={10519--10529},
      year={2024}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2312.10118">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/KAIST-VICLab/From_Ground_To_Objects" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            We thank the authors of <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>  that kindly open sourced the template of this website. 
            Please visit our <a href="https://github.com/KAIST-VICLab">VIC-Lab</a> for more interesting researches
          </p>
        </div>
      </div>
      <a href='https://mapmyvisitors.com/web/1bvq3'  title='Visit tracker'><img src='https://mapmyvisitors.com/map.png?cl=080808&w=a&t=n&d=CSZOLArqehpw0rYkIHHe8k079lbWJz3chOxwETCmBcM&co=ffffff&ct=808080'/></a>
  </div>
</footer>

</body>
</html>
